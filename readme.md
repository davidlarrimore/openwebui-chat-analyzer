# ğŸ’¬ Open WebUI Chat Analyzer

> **A local-first analytics platform for exploring your Open WebUI conversations**

Transform your Open WebUI chat history into actionable insights with this comprehensive analytics stack. Featuring a FastAPI backend paired with a modern Next.js dashboard, your conversation data never leaves your environmentâ€”making it perfect for privacy-conscious teams and individual power users.

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)

---

## âœ¨ Key Features

### ğŸ”’ **Privacy-First Architecture**
- **100% Local Processing** â€“ All data stays on your machine
- **No External Services** â€“ Dashboard communicates only with your local backend
- **Self-Hosted by Design** â€“ Complete control over your conversation analytics
- **Adaptive Alias System** â€“ Stable pseudonyms are stored in the database with an optional real-name override

### ğŸ“Š **Comprehensive Analytics**
- **ğŸ“ˆ Time Analysis** â€“ Daily trends, conversation patterns, hour-by-day heatmaps
- **ğŸ“ Content Analysis** â€“ Word clouds, message length distributions, sentiment breakdown
- **ğŸ’¬ Chat Browser** â€“ Full-text search, filters, and detailed conversation views
- **ğŸ” Advanced Search** â€“ Query across all messages with powerful filtering options

### ğŸš€ **Intelligent Data Loading**
- **Direct Connect** â€“ Sync live from your Open WebUI instance with one click
- **File Import** â€“ Drop exports into `data/` or upload through the UI
- **Instant Metrics** â€“ Dashboard updates immediately while summaries process in background
- **Incremental Sync** â€“ Smart updates that only fetch new conversations

### ğŸ¤– **AI-Powered Summaries**
- **Local LLM Integration** â€“ Uses Ollama for automatic chat summarization
- **Incremental Persistence** â€“ Summaries saved as each chat completes (no data loss)
- **Smart Context** â€“ Sentence transformers identify salient utterances for better summaries
- **Fallback Support** â€“ Can use Open WebUI completions endpoint if needed

### ğŸ¨ **Modern UI/UX**
- **Next.js 14 App Router** â€“ Fast, responsive single-page application
- **Tailwind + shadcn/ui** â€“ Beautiful, accessible component library
- **Real-Time Updates** â€“ Live processing logs and progress tracking
- **Multi-User Support** â€“ Auth.js with credentials and GitHub OAuth

---

## ğŸ¯ Quick Start

### Option A: Docker (Recommended)

```bash
git clone https://github.com/davidlarrimore/openwebui-chat-analyzer.git
cd openwebui-chat-analyzer
cp .env.example .env
make up
```

**Access Points:**
- ğŸ¨ **Dashboard**: http://localhost:8503
- ğŸ”Œ **API**: http://localhost:8502
- ğŸ“– **API Docs**: http://localhost:8502/docs

**Useful Commands:**
```bash
make logs    # View combined logs
make down    # Stop all services
make restart # Restart services
make help    # See all available commands
```

### Option B: Local Development

**Backend:**
```bash
git clone https://github.com/davidlarrimore/openwebui-chat-analyzer.git
cd openwebui-chat-analyzer
python3 -m venv venv && source venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt
cp .env.example .env

# First run only
python -m textblob.download_corpora

# Start backend
uvicorn backend.app:app --reload --port 8502
```

**Frontend** (in a new terminal):
```bash
cd openwebui-chat-analyzer/frontend-next
pnpm install
pnpm dev  # Runs on http://localhost:3000
```

### Option C: Guided Setup

```bash
scripts/setup.sh  # Interactive wizard for Docker or local setup
```

---

## ğŸ“– Dashboard Overview

### ğŸ“Š Overview
- Total conversations, messages, and user activity
- Model usage statistics and file upload tracking
- Approximate token volume (derived from character counts)
- User and model breakdowns with visual charts

### ğŸ“ˆ Time Analysis
- **Daily Trends** â€“ Message volume over time
- **Conversation Length** â€“ Distribution of chat durations
- **Heatmaps** â€“ Activity by hour and day of week
- **Filters** â€“ Segment by user and model

### ğŸ“ Content Analysis
- **Word Clouds** â€“ Most frequently used terms
- **Message Length** â€“ Histograms by role and model
- **Sentiment Breakdown** â€“ Positive, neutral, negative classification
- **Per-User Insights** â€“ Individual communication patterns

### ğŸ” Search
- **Full-Text Search** â€“ Query across all messages
- **Advanced Filters** â€“ By user, model, date range, sentiment
- **Export Results** â€“ Download filtered data as CSV or JSON

### ğŸ’¬ Browse Chats
- **Paginated View** â€“ Browse all conversations
- **Rich Metadata** â€“ Timestamps, participants, model info
- **AI Summaries** â€“ One-line headlines for each chat
- **Quick Actions** â€“ Download individual threads as JSON

### âš™ï¸ Configuration
- **Data Source Management** â€“ Connect to Open WebUI or upload exports
- **Sync Settings** â€“ Configure full vs incremental sync modes
- **Automated Scheduler** â€“ Set up periodic data refreshes
- **Summarizer Settings** â€“ Choose Ollama model for AI summaries
- **Identity Privacy** â€“ Toggle between pseudonyms and real names on user-facing charts
- **Real-Time Logs** â€“ Monitor sync and processing operations
- **System Status** â€“ View connection health and data freshness

---

## ğŸ”§ Configuration

### Environment Setup

Copy `.env.example` to `.env` and configure:

#### Backend Connectivity
```bash
OWUI_API_BASE_URL=http://localhost:8502       # Backend URL for dashboard
OWUI_API_ALLOWED_ORIGINS=http://localhost:3000 # CORS origins
OWUI_DATA_DIR=./data                           # Default export directory
```

#### Direct Connect Defaults
```bash
OWUI_DIRECT_HOST=http://localhost:3000         # Open WebUI base URL
OWUI_DIRECT_API_KEY=                          # Optional prefill API key
OWUI_EXPOSE_REAL_NAMES=false                  # Set true to expose real names by default
```

#### AI & Summarization
```bash
# Sentence Transformers
EMB_MODEL=sentence-transformers/all-MiniLM-L6-v2
SALIENT_K=10                                   # Number of salient utterances

# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_TIMEOUT=180
OLLAMA_DEFAULT_MODEL=llama3.2:latest
OLLAMA_EMBEDDING_MODEL=nomic-embed-text:latest
```

#### Frontend Settings
```bash
FRONTEND_NEXT_PORT=8503                        # Published dashboard port
FRONTEND_NEXT_PUBLIC_URL=http://localhost:8503 # External URL
FRONTEND_NEXT_BACKEND_BASE_URL=http://backend:8502 # Internal backend URL

# Auth.js
NEXTAUTH_SECRET=your-secret-here
NEXTAUTH_URL=http://localhost:8503

# Optional GitHub OAuth
GITHUB_OAUTH_ENABLED=false
GITHUB_CLIENT_ID=
GITHUB_CLIENT_SECRET=
```

### Identity Privacy

- **Pseudonym Catalog** â€“ `backend/data/pseudonyms.json` contains the canonical alias list used when ingesting users.
- **Stable Assignments** â€“ Pseudonyms are persisted in the database and refreshed automatically on every sync.
- **Configurable Exposure** â€“ Toggle between pseudonyms and real names from the âš™ï¸ Configuration page or set `OWUI_EXPOSE_REAL_NAMES=true` to default to real names.

---

## ğŸ“¥ Loading Data

### Method 1: Direct Connect (Recommended)

1. Navigate to **âš™ï¸ Configuration** in the dashboard
2. Click **Edit Credentials** and enter:
   - Your Open WebUI base URL (e.g., `http://localhost:3000`)
   - An API key with read permissions
3. Click **Test Connection** to verify
4. Click **Sync Data Now** to import

**Benefits:**
- âœ… Automatic incremental updates
- âœ… Always in sync with your Open WebUI instance
- âœ… No manual export/import workflow
- âœ… Scheduler support for automated syncs

### Method 2: File Upload

1. Export from Open WebUI:
   - **Settings â†’ Data & Privacy â†’ Export All Chats** (`all-chats-export-*.json`)
   - **Settings â†’ Database â†’ Export Users** (`users.csv`, optional)
   - Capture `/api/v1/models` as `models.json` (optional, for friendly names)

2. Import options:
   - Drop files in the `data/` directory (auto-loaded on startup)
   - Upload through **âš™ï¸ Configuration** page
   - Files stored in `uploads/` directory

---

## ğŸ¤– AI Summaries

### How It Works

The analyzer automatically generates one-line summaries for each conversation:

1. **Salient Extraction** â€“ Uses sentence-transformers to identify key utterances
2. **LLM Summarization** â€“ Feeds context to your configured Ollama model
3. **Incremental Persistence** â€“ Saves each summary immediately (no data loss)
4. **Background Processing** â€“ Metrics update instantly; summaries generate async

### Configuration

Choose your summarization model in **âš™ï¸ Configuration â†’ Summarizer Settings**:
- Select from available Ollama models
- Settings persist to database
- Changes apply to future summarization jobs

### Rebuilding Summaries

Regenerate all summaries anytime:
- Click **âš™ï¸ Configuration â†’ Quick Actions â†’ Rebuild Summaries**
- Or via API: `POST /api/v1/summaries/rebuild`
- Monitor progress in the **Processing Log**

---

## ğŸ”„ Sync Modes

### Full Sync
- **When to Use**: First sync, changing data sources, or recovering from issues
- **What It Does**: Replaces all local data with fresh import from Open WebUI
- **Recommended**: When `has_data: false` or hostname changes

### Incremental Sync
- **When to Use**: Regular updates from the same Open WebUI instance
- **What It Does**: Fetches only new conversations since last sync
- **Recommended**: When `has_data: true` and source matches
- **Benefits**: Faster, preserves local summaries, efficient

The dashboard **automatically recommends** the appropriate mode based on your current dataset state.

---

## ğŸ”Œ Backend API

Key endpoints for integration and automation:

### Dataset & Metadata
```bash
GET  /api/v1/datasets/meta          # Current dataset stats
GET  /api/v1/chats                  # Chat metadata
GET  /api/v1/messages               # Message content
GET  /api/v1/users                  # User directory
POST /api/v1/datasets/reset         # Delete all data
```

### Direct Connect
```bash
POST /api/v1/openwebui/sync         # Sync from Open WebUI
POST /api/v1/openwebui/test         # Test connection
GET  /api/v1/sync/status            # Sync status & freshness
```

### File Uploads
```bash
POST /api/v1/uploads/chat-export    # Upload all-chats-export.json
POST /api/v1/uploads/users          # Upload users.csv
POST /api/v1/uploads/models         # Upload models.json
```

### Summaries
```bash
GET  /api/v1/summaries/status       # Current summarizer status
POST /api/v1/summaries/rebuild      # Regenerate all summaries
GET  /api/v1/summaries/events       # Stream summary events
```

### Admin Settings
```bash
GET  /api/v1/admin/settings/direct-connect     # Get Direct Connect settings
PUT  /api/v1/admin/settings/direct-connect     # Update settings
GET  /api/v1/sync/scheduler                    # Get scheduler config
POST /api/v1/sync/scheduler                    # Update scheduler
```

**Interactive API Docs**: Visit http://localhost:8502/docs when backend is running

---

## ğŸ“Š Data Export

### CSV Downloads
- **What's Included**: Same columns shown in dashboard tables
- **Use Cases**: Analysis in Excel, pandas, Tableau, Power BI
- **Fields**: Timestamps, participants, sentiment scores, token estimates

### JSON Downloads
- **What's Included**: Complete conversation metadata and messages
- **Format**: ISO timestamps, attachments, role information
- **Use Cases**: Backup, data migration, custom processing

### Notes
- Token estimates are **heuristic** (based on character counts)
- Sentiment scores use **TextBlob** polarity scale (âˆ’1 to 1)
- Exports reflect current filter/search state

---

## ğŸ§ª Sample Data

Explore the dashboard instantly with sample data:

```bash
cp sample_data/sample_data_extract.json data/
cp sample_data/sample_users.csv data/
# Restart backend to auto-load, or upload via Configuration page
```

---

## ğŸ—ï¸ Project Structure

```
openwebui-chat-analyzer/
â”œâ”€â”€ backend/                 # FastAPI application
â”‚   â”œâ”€â”€ app.py              # Main application entry point
â”‚   â”œâ”€â”€ routes.py           # API endpoint definitions
â”‚   â”œâ”€â”€ services.py         # Business logic & data processing
â”‚   â”œâ”€â”€ db.py               # SQLite database layer
â”‚   â”œâ”€â”€ models.py           # Pydantic models
â”‚   â”œâ”€â”€ summarizer.py       # AI summarization pipeline
â”‚   â””â”€â”€ tests/              # Backend test suite
â”‚
â”œâ”€â”€ frontend-next/          # Next.js 14 dashboard
â”‚   â”œâ”€â”€ app/                # App Router pages & layouts
â”‚   â”œâ”€â”€ components/         # React components
â”‚   â”œâ”€â”€ lib/                # Utilities, types, API client
â”‚   â””â”€â”€ tests/              # Frontend test suite
â”‚
â”œâ”€â”€ data/                   # Default export directory
â”œâ”€â”€ uploads/                # User-uploaded files
â”œâ”€â”€ scripts/                # Setup & utility scripts
â”œâ”€â”€ sample_data/            # Example datasets
â”œâ”€â”€ .env.example            # Environment template
â””â”€â”€ docker-compose.yml      # Container orchestration
```

---

## ğŸ” Privacy & Security

### Data Handling
- âœ… **100% Local** â€“ All processing happens on your machine
- âœ… **No External Calls** â€“ Dashboard only talks to local backend
- âœ… **No Telemetry** â€“ Zero tracking or analytics collection
- âœ… **File-Based Storage** â€“ SQLite database in your project directory

### Credential Management
- ğŸ”’ API keys stored in database with quote-safe normalization
- ğŸ”’ Password fields in UI (`type="password"`)
- ğŸ”’ Redacted logging (shows `supe...2345` instead of full key)
- ğŸ”’ Keys never appear in processing logs or responses

### Authentication
- Auth.js (NextAuth) with local credentials provider
- Optional GitHub OAuth integration
- Session-based authentication
- Protected API routes

---

## ğŸ§© Advanced Features

### Automatic Sync Scheduler
- Configure periodic incremental syncs (5 min to 24 hours)
- Enable/disable via **âš™ï¸ Configuration â†’ Scheduler**
- Settings persist across restarts
- Runs in background without blocking dashboard

### Data Freshness Indicators
- **Staleness Threshold**: Configurable via `SYNC_STALENESS_THRESHOLD_HOURS` (default: 6 hours)
- **Visual Pills**: Green "Current" / Amber "Stale" indicators
- **Last Sync Display**: Human-readable timestamps with relative time

### Processing Log Viewer
- **Real-Time Streaming**: Polls `/api/v1/logs` every 2 seconds
- **Auto-Scroll**: Follows new entries (disable by scrolling up)
- **Structured Logs**: Timestamp, level, phase, job ID, message, details
- **Circular Buffer**: Retains last 200 events
- **Color-Coded Levels**: Debug, info, warning, error

### WAL Mode & Performance
- SQLite Write-Ahead Logging for better concurrency
- Foreign key enforcement for data integrity
- Normal synchronous mode for speed with safety
- Prevents long locks during large syncs

---

## ğŸ› ï¸ Development

### Frontend Development
```bash
cd frontend-next
pnpm dev        # Start dev server (http://localhost:3000)
pnpm build      # Production build
pnpm test       # Run test suite
pnpm lint       # ESLint check
```

### Backend Development
```bash
source venv/bin/activate
uvicorn backend.app:app --reload --port 8502  # Auto-reload on changes
pytest backend/tests/                          # Run tests
```

### Docker Development
```bash
make dev        # Hot-reload for both frontend and backend
make logs       # Tail all container logs
make shell      # Access backend container shell
```

### Testing
```bash
# Backend
pytest backend/tests/ -v

# Frontend
cd frontend-next && pnpm test
```

---

## ğŸ› Troubleshooting

### Dashboard Won't Start
- âœ… Verify Node.js 20+ is installed: `node --version`
- âœ… Verify pnpm is installed: `pnpm --version`
- âœ… Clear cache: `pnpm store prune`

### Backend Connection Issues
- âœ… Confirm backend is running: `curl http://localhost:8502/health`
- âœ… Check `OWUI_API_BASE_URL` in `.env`
- âœ… Verify no port conflicts: `lsof -i :8502`

### Summarizer Failing
- âœ… Confirm Ollama is running: `ollama list`
- âœ… Verify model is available: `ollama run llama3.2:latest`
- âœ… Check `OLLAMA_BASE_URL` in `.env`
- âœ… Increase timeout: `OLLAMA_TIMEOUT=300`

### Word Clouds Not Rendering
- âœ… Install system fonts: `sudo apt-get install fonts-dejavu`
- âœ… Restart backend after font installation

### Database Locked Errors
- âœ… Ensure only one backend instance is running
- âœ… Check for stale lock files in database directory
- âœ… Verify WAL mode is enabled (automatic in recent versions)

### Sync Shows "Stale" Data
- âœ… Run manual sync from **âš™ï¸ Configuration**
- âœ… Adjust `SYNC_STALENESS_THRESHOLD_HOURS` if needed
- âœ… Enable automatic scheduler for regular updates

---

## ğŸ¤ Contributing

Contributions are welcome! Please review [AGENTS.md](AGENTS.md) for:
- Coding standards and conventions
- Development workflow guidelines
- Testing requirements
- Release procedures

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

Built with:
- [Next.js](https://nextjs.org/) - React framework
- [FastAPI](https://fastapi.tiangolo.com/) - Python web framework
- [Tailwind CSS](https://tailwindcss.com/) - Utility-first CSS
- [shadcn/ui](https://ui.shadcn.com/) - Component library
- [Auth.js](https://authjs.dev/) - Authentication
- [Ollama](https://ollama.ai/) - Local LLM runtime
- [Sentence Transformers](https://www.sbert.net/) - Semantic embeddings
- [TextBlob](https://textblob.readthedocs.io/) - Sentiment analysis

---

## ğŸ“¬ Support

- ğŸ› **Bug Reports**: [GitHub Issues](https://github.com/davidlarrimore/openwebui-chat-analyzer/issues)
- ğŸ’¡ **Feature Requests**: [GitHub Discussions](https://github.com/davidlarrimore/openwebui-chat-analyzer/discussions)
- ğŸ“– **Documentation**: This README and inline code comments

---

<div align="center">

**Made with â¤ï¸ for the Open WebUI community**

[â¬† Back to Top](#-open-webui-chat-analyzer)

</div>
