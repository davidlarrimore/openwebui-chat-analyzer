services:
  backend:
    build:
      context: .
      dockerfile: Dockerfile
      target: backend
    container_name: openwebui-chat-analyzer-backend
    command: ["uvicorn", "backend.app:app", "--host", "0.0.0.0", "--port", "8502"]
    ports:
      - "8502:8502"
    volumes:
      - ./backend:/app/backend:ro
      - ./backend/data:/app/backend/data:rw
      - ./data:/app/data:rw
    env_file:
      - .env
    environment:
      OLLAMA_BASE_URL: http://host.docker.internal:11434
      OWUI_LOG_LEVEL: ${OWUI_LOG_LEVEL:-INFO}
      AUTH_MODE: ${AUTH_MODE:-HYBRID}
      SESSION_SECRET: ${SESSION_SECRET:-super-secret-change-me}
      SESSION_COOKIE_NAME: ${SESSION_COOKIE_NAME:-analyzer_session}
      SESSION_COOKIE_SECURE: ${SESSION_COOKIE_SECURE:-true}
      SESSION_COOKIE_SAMESITE: ${SESSION_COOKIE_SAMESITE:-Lax}
      APP_BASE_URL: ${APP_BASE_URL:-http://localhost:${FRONTEND_NEXT_PORT:-8503}}
      OIDC_CLIENT_ID: ${OIDC_CLIENT_ID:-}
      OIDC_CLIENT_SECRET: ${OIDC_CLIENT_SECRET:-}
      OIDC_ISSUER: ${OIDC_ISSUER:-}
      TZ: ${TZ:-UTC}
    healthcheck:
      test:
        ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://127.0.0.1:8502/health', timeout=5)"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 5s
    restart: unless-stopped

  frontend-next:
    build:
      context: ./frontend-next
      args:
        BACKEND_BASE_URL: ${FRONTEND_NEXT_BACKEND_BASE_URL:-http://backend:8502}
        APP_BASE_URL: ${APP_BASE_URL:-http://localhost:${FRONTEND_NEXT_PORT:-8503}}
    container_name: openwebui-chat-analyzer-frontend-next
    ports:
      - "${FRONTEND_NEXT_PORT:-8503}:3000"
    environment:
      - NODE_ENV=production
      - BACKEND_BASE_URL=${FRONTEND_NEXT_BACKEND_BASE_URL:-http://backend:8502}
      - APP_BASE_URL=${APP_BASE_URL:-http://localhost:${FRONTEND_NEXT_PORT:-8503}}
      - PORT=3000
    depends_on:
      backend:
        condition: service_healthy
    restart: unless-stopped

  nginx:
    image: nginx:alpine
    container_name: openwebui-chat-analyzer-proxy
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
    depends_on:
      frontend-next:
        condition: service_started
    restart: unless-stopped
    profiles:
      - production

volumes:
  frontend_next_node_modules:
