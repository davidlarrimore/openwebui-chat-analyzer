services:
  postgres:
    image: postgres:16-alpine
    container_name: openwebui-chat-analyzer-db
    environment:
      - POSTGRES_DB=${OWUI_DB_NAME:-openwebui_chat_analyzer}
      - POSTGRES_USER=${OWUI_DB_USER:-owui}
      - POSTGRES_PASSWORD=${OWUI_DB_PASSWORD:-owui_password}
    ports:
      - "${OWUI_DB_PORT:-5432}:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${OWUI_DB_USER:-owui} -d ${OWUI_DB_NAME:-openwebui_chat_analyzer}"]
      interval: 5s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  ollama:
    image: ollama/ollama:latest
    container_name: openwebui-chat-analyzer-ollama
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    env_file:
      - .env
    environment:
      - OLLAMA_KEEP_ALIVE=${OLLAMA_KEEP_ALIVE:-24h}
      - OLLAMA_MAX_LOADED_MODELS=${OLLAMA_MAX_LOADED_MODELS:-4}
      - OLLAMA_PRELOAD_MODELS=${OLLAMA_PRELOAD_MODELS:-}
    volumes:
      - ./scripts/ollama-bootstrap.sh:/usr/local/bin/ollama-bootstrap.sh:ro
      - ollama_data:/root/.ollama
    entrypoint: ["/bin/sh", "/usr/local/bin/ollama-bootstrap.sh"]
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  frontend:
    build:
      context: .
      dockerfile: Dockerfile
      target: frontend
    container_name: openwebui-chat-analyzer-frontend
    ports:
      - "8501:8501"
    volumes:
      - ./frontend:/app/frontend:ro
      - ./data:/app/data:rw
      - ./uploads:/app/uploads:rw
      - ./.streamlit:/app/.streamlit:ro
    env_file:
      - .env
    environment:
      - STREAMLIT_SERVER_PORT=${STREAMLIT_SERVER_PORT:-8501}
      - STREAMLIT_SERVER_ADDRESS=${STREAMLIT_SERVER_ADDRESS:-0.0.0.0}
      - STREAMLIT_SERVER_HEADLESS=${STREAMLIT_SERVER_HEADLESS:-true}
      - STREAMLIT_BROWSER_GATHER_USAGE_STATS=${STREAMLIT_BROWSER_GATHER_USAGE_STATS:-false}
      - STREAMLIT_SERVER_FILE_WATCHER_TYPE=${STREAMLIT_SERVER_FILE_WATCHER_TYPE:-none}
      - MPLCONFIGDIR=${MPLCONFIGDIR:-/tmp/matplotlib}
      - TZ=${TZ:-UTC}
      - OWUI_API_BASE_URL=http://backend:8502
    depends_on:
      backend:
        condition: service_started
    restart: unless-stopped

  backend:
    build:
      context: .
      dockerfile: Dockerfile
      target: backend
    container_name: openwebui-chat-analyzer-backend
    command: uvicorn backend.app:app --host ${OWUI_API_HOST:-0.0.0.0} --port ${OWUI_API_PORT:-8502}
    ports:
      - "8502:8502"
    volumes:
      - ./backend:/app/backend:ro
      - ./data:/app/data:rw
    env_file:
      - .env
    environment:
      - TZ=${TZ:-UTC}
    restart: unless-stopped
    depends_on:
      ollama:
        condition: service_healthy
      postgres:
        condition: service_healthy

  frontend-next:
    build:
      context: ./frontend-next
    container_name: openwebui-chat-analyzer-frontend-next
    ports:
      - "${FRONTEND_NEXT_PORT:-8503}:3000"
    environment:
      - NODE_ENV=production
      - BACKEND_BASE_URL=${FRONTEND_NEXT_BACKEND_BASE_URL:-http://backend:8502}
      - NEXTAUTH_SECRET=${NEXTAUTH_SECRET:-dev-secret}
      - NEXTAUTH_URL=${FRONTEND_NEXT_PUBLIC_URL:-http://localhost:${FRONTEND_NEXT_PORT:-8503}}
      - GITHUB_OAUTH_ENABLED=${GITHUB_OAUTH_ENABLED:-false}
      - NEXT_PUBLIC_GITHUB_OAUTH_ENABLED=${NEXT_PUBLIC_GITHUB_OAUTH_ENABLED:-false}
      - GITHUB_CLIENT_ID=${GITHUB_CLIENT_ID:-}
      - GITHUB_CLIENT_SECRET=${GITHUB_CLIENT_SECRET:-}
      - PORT=3000
    depends_on:
      backend:
        condition: service_started
    restart: unless-stopped

  nginx:
    image: nginx:alpine
    container_name: openwebui-chat-analyzer-proxy
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
    depends_on:
      frontend:
        condition: service_started
    restart: unless-stopped
    profiles:
      - production

volumes:
  frontend_next_node_modules:
  postgres_data:
  ollama_data:
