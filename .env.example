# OWUI_DIRECT_HOST: Default Open WebUI base URL shown in the Direct Connect form.
OWUI_DIRECT_HOST=http://localhost:4000
# OWUI_DIRECT_API_KEY: Optional API token that pre-populates the Direct Connect form.
OWUI_DIRECT_API_KEY=your-openwebui-api-key

# SALIENT_K: Number of salient chat utterances to consider when building context.
SALIENT_K=10

# EMB_MODEL: Sentence-transformer used for selecting salient chat messages.
EMB_MODEL=sentence-transformers/all-MiniLM-L6-v2

# OLLAMA_BASE_URL: Base URL for the external Ollama service used by summarization and GenAI routes.
# When running via docker-compose the backend uses http://host.docker.internal:11434 to reach the host runtime; keep http://localhost:11434 for bare-metal runs.
OLLAMA_BASE_URL=http://localhost:11434

# ============================================================================
# OpenAI Configuration
# ============================================================================
# OPENAI_API_KEY: API key for OpenAI services (required to enable OpenAI as a summarization provider).
# Leave empty to disable OpenAI integration.
OPENAI_API_KEY=

# OPENAI_API_BASE: Base URL for OpenAI API endpoints.
# Default: https://api.openai.com/v1
# Override for OpenAI-compatible services (e.g., Azure OpenAI).
OPENAI_API_BASE=https://api.openai.com/v1

# ============================================================================
# LiteLLM Configuration
# ============================================================================
# LITELLM_API_KEY: API key for LiteLLM proxy service (required to enable LiteLLM as a summarization provider).
# Leave empty to disable LiteLLM integration.
LITELLM_API_KEY=

# LITELLM_API_BASE: Base URL for LiteLLM proxy endpoints.
# Default: http://localhost:4000
# LiteLLM acts as a unified proxy/router to multiple LLM providers (OpenAI, Anthropic, Azure, etc.).
LITELLM_API_BASE=http://localhost:4000

# ============================================================================
# Summarizer Configuration (Sprint 1: Enhanced Retry & Error Handling)
# ============================================================================

# Exponential Backoff Configuration
# SUMMARIZER_USE_EXPONENTIAL_BACKOFF: Enable exponential backoff for transient errors (default: true)
# When enabled, retry delays increase exponentially with jitter to prevent thundering herd
SUMMARIZER_USE_EXPONENTIAL_BACKOFF=true
# SUMMARIZER_RETRY_MAX_ATTEMPTS: Maximum retry attempts for transient failures (default: 5)
SUMMARIZER_RETRY_MAX_ATTEMPTS=5
# SUMMARIZER_RETRY_BASE_DELAY: Base delay in seconds for exponential backoff (default: 1.0)
SUMMARIZER_RETRY_BASE_DELAY=1.0
# SUMMARIZER_RETRY_MAX_DELAY: Maximum delay cap in seconds (default: 60.0)
SUMMARIZER_RETRY_MAX_DELAY=60.0

# Parse Retry Configuration
# SUMMARIZER_PARSE_RETRY_ATTEMPTS: Number of retry attempts for JSON parsing failures (default: 2)
# On retry, enables JSON mode (if supported by provider) and simplifies prompt
SUMMARIZER_PARSE_RETRY_ATTEMPTS=2

# Error Logging Configuration
# SUMMARIZER_PRESERVE_FULL_ERRORS: Preserve full prompts and responses for debugging (default: true)
# When enabled, captures complete error context (useful for troubleshooting but can be large)
SUMMARIZER_PRESERVE_FULL_ERRORS=true
# SUMMARIZER_MAX_ERROR_SIZE: Maximum size for preserved errors in characters (default: 10000)
# Prevents memory issues by capping error detail size
SUMMARIZER_MAX_ERROR_SIZE=10000

# Legacy retry configuration (for backward compatibility with pre-Sprint 1 configs)
# SUMMARIZER_OLLAMA_RETRY_ATTEMPTS: Fixed retry attempts for Ollama connections (default: 3)
# SUMMARIZER_OLLAMA_RETRY_DELAY_SECONDS: Fixed delay between retries in seconds (default: 3.0)
# Note: These are superseded by exponential backoff when SUMMARIZER_USE_EXPONENTIAL_BACKOFF=true
SUMMARIZER_OLLAMA_RETRY_ATTEMPTS=3
SUMMARIZER_OLLAMA_RETRY_DELAY_SECONDS=3.0

# FRONTEND_NEXT_PORT: Published port for the Next.js dashboard container.
FRONTEND_NEXT_PORT=8503
# FRONTEND_NEXT_BACKEND_BASE_URL: Internal URL the Next.js dashboard uses to reach FastAPI.
FRONTEND_NEXT_BACKEND_BASE_URL=http://backend:8502

# Authentication configuration
AUTH_MODE=HYBRID
SESSION_SECRET=replace-with-secure-random-string
SESSION_COOKIE_NAME=analyzer_session
SESSION_COOKIE_SECURE=false
SESSION_COOKIE_SAMESITE=Lax
APP_BASE_URL=http://localhost:8503
BACKEND_BASE_URL=http://backend:8502
# Microsoft 365 / Entra ID (OIDC) configuration
OIDC_ISSUER=https://login.microsoftonline.com/<tenant-id-guid>/v2.0
OIDC_CLIENT_ID=
OIDC_CLIENT_SECRET=

# Database configuration uses a project-local SQLite file. Override the path as needed.
OWUI_SQLITE_PATH=data/openwebui_chat_analyzer.db
# Alternatively, point to a managed database (PostgreSQL, etc.).
# OWUI_DB_URL=postgresql+psycopg2://owui:owui_password@postgres:5432/openwebui_chat_analyzer
# OWUI_OPENWEBUI_TIMEOUT_SECONDS: Request timeout when fetching from Open WebUI (seconds).
OWUI_OPENWEBUI_TIMEOUT_SECONDS=45

# Logging configuration for the backend service.
OWUI_LOG_LEVEL=INFO
# Allow local Next.js origins to call the API.
OWUI_API_ALLOWED_ORIGINS=http://localhost:3000,http://127.0.0.1:3000,http://localhost:8503
